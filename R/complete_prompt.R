#' Complete a Custom GPT-3 Prompt
#'
#' @param prompt The prompt to use as input for GPT-3
#' @param model  Which model variant of GPT-3 to use. Defaults to 'davinci-002'
#' @param openai_api_key Your API key. By default, looks for a system environment variable called "OPENAI_API_KEY" (recommended option). Otherwise, it will prompt you to enter the API key as an argument.
#' @param max_tokens How many tokens (roughly 4 characters of text) should GPT-3 return? Defaults to a single token (next word prediction).
#' @param temperature A number between 0 and 100. When set to zero, GPT-3 will always return the most probable next token. When set higher, GPT-3 will select the next word probabilistically.
#'
#' @return If max_tokens is 1, returns a dataframe with the 5 most likely next words and their probabilities as assigned by GPT-3. If max_tokens > 1, returns a single string of text generated by GPT-3.
#' @export
#'
#' @examples
#' complete_prompt('I feel like a')
#' complete_prompt('Write a haiku about frogs.', max_tokens = 100)
complete_prompt <- function(prompt,
                            model = 'davinci-002',
                            openai_api_key = Sys.getenv('OPENAI_API_KEY'),
                            max_tokens = 1,
                            temperature = 0) {


  openai <- reticulate::import("openai")

  if(openai_api_key == ''){
    stop("No API key detected in system environment. You can enter it manually using the 'openai_api_key' argument.")
  } else{
    openai$api_key = openai_api_key
  }

  # query the API

  if(model %in% c('gpt-3.5-turbo', 'gpt-3.5-turbo-16k')){
    # for Chat Endpoint, embed the prompt in a "messages" dictionary object
    messages <- reticulate::dict(role = 'user',
                                 content = prompt)

    # query the API
    response <- openai$ChatCompletion$create(model = model,
                                             messages = c(messages),
                                             temperature = as.integer(temperature),
                                             max_tokens = as.integer(max_tokens))

    return(response$choices[[1]]$message$content)

  } else{

    response <- openai$Completion$create(
      engine = model,
      prompt = prompt,
      logprobs = ifelse(max_tokens == 1, as.integer(5), as.integer(1)),
      max_tokens = as.integer(max_tokens),
      temperature = as.integer(temperature)
    )

    # if user requests more than one token, returns a single autoregressively generated response from GPT-3
    if(max_tokens > 1){
      return(response$choices[[1]]$text)
    } else{
      # if user requests 1 token (default), return the vector of next word predictions
      # and their associated log probabilities

      # convert the list of log probabilities into a dataframe
      keys <- names(response$choices[[1]]$logprobs$top_logprobs[[1]])

      logprobs <- as.numeric(response$choices[[1]]$logprobs$top_logprobs[[1]])

      data.frame(response = keys,
                 prob = exp(logprobs))
    }

  }

}
